# EVA4 Assignment 8 - Praveen Raghuvanshi



- [Github link](https://github.com/praveenraghuvanshi1512/EVA4/blob/Session-8/Session-8/Assignment-8/EVA_4_S8_Praveen_Raghuvanshi_Main.ipynb) 

- [Colab link](https://colab.research.google.com/drive/1Y2tUxeeqe0JptuWS3TFjsLvmoNPzCbjH?authuser=1#scrollTo=mmvSUlfTa1j9)

- Model: Resnet18

- No of parameters: 11,173,962

- No of Epochs : 30

- Best Test Acc: 91.110%

- Code Modularity

  - EVA_4_S8_Praveen_Raghuvanshi_Main.ipynb : It contains main workflow
  - S8_functions.py: It contains all the functions related to setup, train, load dataset etc.
  - model.py : It contains Resnet model
  - Utils.py : It contains helper functions such as progress bar

- Logs

  - Best accuracy

    ```python
    Epoch: 29
     [================================================================>]  Step: 45ms | Tot: 10m39s | Train >> Loss: 0.119 | Acc: 95.856% (47928/50000) 12500/12500 
     [================================================================>]  Step: 21ms | Tot: 58s597ms | Test >> Loss: 0.297 | Acc: 91.110% (9111/10000) 2500/2500 
    
    ```

  - Full

    ```python
    
    Epoch: 0
     [================================================================>]  Step: 52ms | Tot: 10m48s | Train >> Loss: 1.573 | Acc: 43.288% (21644/50000) 12500/12500 
     [================================================================>]  Step: 21ms | Tot: 59s966ms | Test >> Loss: 1.146 | Acc: 60.500% (6050/10000) 2500/2500 
    
    Epoch: 1
     [================================================================>]  Step: 50ms | Tot: 10m43s | Train >> Loss: 1.003 | Acc: 64.644% (32322/50000) 12500/12500 
     [================================================================>]  Step: 27ms | Tot: 1m744ms | Test >> Loss: 0.791 | Acc: 72.120% (7212/10000) 2500/2500 
    
    Epoch: 2
     [================================================================>]  Step: 48ms | Tot: 10m43s | Train >> Loss: 0.780 | Acc: 72.986% (36493/50000) 12500/12500 
     [================================================================>]  Step: 23ms | Tot: 58s701ms | Test >> Loss: 0.689 | Acc: 76.170% (7617/10000) 2500/2500 
    
    Epoch: 3
     [================================================================>]  Step: 50ms | Tot: 10m41s | Train >> Loss: 0.658 | Acc: 77.382% (38691/50000) 12500/12500 
     [================================================================>]  Step: 19ms | Tot: 57s638ms | Test >> Loss: 0.569 | Acc: 80.710% (8071/10000) 2500/2500 
    
    Epoch: 4
     [================================================================>]  Step: 51ms | Tot: 10m40s | Train >> Loss: 0.579 | Acc: 80.060% (40030/50000) 12500/12500 
     [================================================================>]  Step: 20ms | Tot: 58s840ms | Test >> Loss: 0.513 | Acc: 82.900% (8290/10000) 2500/2500 
    
    Epoch: 5
     [================================================================>]  Step: 50ms | Tot: 10m47s | Train >> Loss: 0.517 | Acc: 82.128% (41064/50000) 12500/12500 
     [================================================================>]  Step: 22ms | Tot: 1m669ms | Test >> Loss: 0.501 | Acc: 83.080% (8308/10000) 2500/2500 
    
    Epoch: 6
     [================================================================>]  Step: 48ms | Tot: 10m46s | Train >> Loss: 0.470 | Acc: 83.652% (41826/50000) 12500/12500 
     [================================================================>]  Step: 24ms | Tot: 1m314ms | Test >> Loss: 0.465 | Acc: 84.380% (8438/10000) 2500/2500 
    
    Epoch: 7
     [================================================================>]  Step: 51ms | Tot: 10m46s | Train >> Loss: 0.429 | Acc: 85.306% (42653/50000) 12500/12500 
     [================================================================>]  Step: 29ms | Tot: 59s404ms | Test >> Loss: 0.440 | Acc: 85.320% (8532/10000) 2500/2500 
    
    Epoch: 8
     [================================================================>]  Step: 52ms | Tot: 10m47s | Train >> Loss: 0.398 | Acc: 86.496% (43248/50000) 12500/12500 
     [================================================================>]  Step: 20ms | Tot: 58s904ms | Test >> Loss: 0.419 | Acc: 85.920% (8592/10000) 2500/2500 
    
    Epoch: 9
     [================================================================>]  Step: 50ms | Tot: 10m46s | Train >> Loss: 0.370 | Acc: 87.090% (43545/50000) 12500/12500 
     [================================================================>]  Step: 22ms | Tot: 57s843ms | Test >> Loss: 0.412 | Acc: 86.230% (8623/10000) 2500/2500 
    
    Epoch: 10
     [================================================================>]  Step: 53ms | Tot: 10m44s | Train >> Loss: 0.344 | Acc: 88.238% (44119/50000) 12500/12500 
     [================================================================>]  Step: 20ms | Tot: 58s787ms | Test >> Loss: 0.435 | Acc: 85.770% (8577/10000) 2500/2500 
    
    Epoch: 11
     [================================================================>]  Step: 47ms | Tot: 10m46s | Train >> Loss: 0.323 | Acc: 88.856% (44428/50000) 12500/12500 
     [================================================================>]  Step: 21ms | Tot: 58s816ms | Test >> Loss: 0.389 | Acc: 87.370% (8737/10000) 2500/2500 
    
    Epoch: 12
     [================================================================>]  Step: 50ms | Tot: 10m40s | Train >> Loss: 0.300 | Acc: 89.536% (44768/50000) 12500/12500 
     [================================================================>]  Step: 24ms | Tot: 56s582ms | Test >> Loss: 0.394 | Acc: 86.680% (8668/10000) 2500/2500 
    
    Epoch: 13
     [================================================================>]  Step: 43ms | Tot: 10m42s | Train >> Loss: 0.283 | Acc: 90.152% (45076/50000) 12500/12500 
     [================================================================>]  Step: 20ms | Tot: 58s964ms | Test >> Loss: 0.397 | Acc: 86.930% (8693/10000) 2500/2500 
    
    Epoch: 14
     [================================================================>]  Step: 50ms | Tot: 10m43s | Train >> Loss: 0.263 | Acc: 90.850% (45425/50000) 12500/12500 
     [================================================================>]  Step: 22ms | Tot: 1m77ms | Test >> Loss: 0.324 | Acc: 89.380% (8938/10000) 2500/2500 
    
    Epoch: 15
     [================================================================>]  Step: 56ms | Tot: 10m45s | Train >> Loss: 0.250 | Acc: 91.286% (45643/50000) 12500/12500 
     [================================================================>]  Step: 24ms | Tot: 59s375ms | Test >> Loss: 0.358 | Acc: 88.140% (8814/10000) 2500/2500 
    
    Epoch: 16
     [================================================================>]  Step: 48ms | Tot: 10m49s | Train >> Loss: 0.237 | Acc: 91.794% (45897/50000) 12500/12500 
     [================================================================>]  Step: 20ms | Tot: 1m920ms | Test >> Loss: 0.323 | Acc: 88.990% (8899/10000) 2500/2500 
    
    Epoch: 17
     [================================================================>]  Step: 51ms | Tot: 10m44s | Train >> Loss: 0.224 | Acc: 92.196% (46098/50000) 12500/12500 
     [================================================================>]  Step: 19ms | Tot: 59s92ms | Test >> Loss: 0.320 | Acc: 89.500% (8950/10000) 2500/2500 
    
    Epoch: 18
     [================================================================>]  Step: 50ms | Tot: 10m45s | Train >> Loss: 0.211 | Acc: 92.644% (46322/50000) 12500/12500 
     [================================================================>]  Step: 21ms | Tot: 58s | Test >> Loss: 0.316 | Acc: 89.720% (8972/10000) 2500/2500 
    
    Epoch: 19
     [================================================================>]  Step: 51ms | Tot: 10m43s | Train >> Loss: 0.199 | Acc: 93.112% (46556/50000) 12500/12500 
     [================================================================>]  Step: 21ms | Tot: 58s548ms | Test >> Loss: 0.320 | Acc: 89.850% (8985/10000) 2500/2500 
    
    Epoch: 20
     [================================================================>]  Step: 49ms | Tot: 10m41s | Train >> Loss: 0.188 | Acc: 93.466% (46733/50000) 12500/12500 
     [================================================================>]  Step: 25ms | Tot: 59s857ms | Test >> Loss: 0.302 | Acc: 90.190% (9019/10000) 2500/2500 
    
    Epoch: 21
     [================================================================>]  Step: 51ms | Tot: 10m41s | Train >> Loss: 0.177 | Acc: 93.880% (46940/50000) 12500/12500 
     [================================================================>]  Step: 20ms | Tot: 59s145ms | Test >> Loss: 0.303 | Acc: 90.450% (9045/10000) 2500/2500 
    
    Epoch: 22
     [================================================================>]  Step: 50ms | Tot: 10m42s | Train >> Loss: 0.164 | Acc: 94.298% (47149/50000) 12500/12500 
     [================================================================>]  Step: 20ms | Tot: 59s376ms | Test >> Loss: 0.301 | Acc: 90.710% (9071/10000) 2500/2500 
    
    Epoch: 23
     [================================================================>]  Step: 50ms | Tot: 10m41s | Train >> Loss: 0.161 | Acc: 94.276% (47138/50000) 12500/12500 
     [================================================================>]  Step: 23ms | Tot: 57s858ms | Test >> Loss: 0.339 | Acc: 89.810% (8981/10000) 2500/2500 
    
    Epoch: 24
     [================================================================>]  Step: 50ms | Tot: 10m39s | Train >> Loss: 0.154 | Acc: 94.572% (47286/50000) 12500/12500 
     [================================================================>]  Step: 19ms | Tot: 57s930ms | Test >> Loss: 0.300 | Acc: 90.500% (9050/10000) 2500/2500 
    
    Epoch: 25
     [================================================================>]  Step: 52ms | Tot: 10m40s | Train >> Loss: 0.144 | Acc: 94.982% (47491/50000) 12500/12500 
     [================================================================>]  Step: 22ms | Tot: 1m531ms | Test >> Loss: 0.314 | Acc: 90.460% (9046/10000) 2500/2500 
    
    Epoch: 26
     [================================================================>]  Step: 50ms | Tot: 10m42s | Train >> Loss: 0.137 | Acc: 95.192% (47596/50000) 12500/12500 
     [================================================================>]  Step: 18ms | Tot: 57s380ms | Test >> Loss: 0.310 | Acc: 90.720% (9072/10000) 2500/2500 
    
    Epoch: 27
     [================================================================>]  Step: 51ms | Tot: 10m41s | Train >> Loss: 0.129 | Acc: 95.410% (47705/50000) 12500/12500 
     [================================================================>]  Step: 25ms | Tot: 58s971ms | Test >> Loss: 0.316 | Acc: 90.310% (9031/10000) 2500/2500 
    
    Epoch: 28
     [================================================================>]  Step: 58ms | Tot: 10m41s | Train >> Loss: 0.122 | Acc: 95.676% (47838/50000) 12500/12500 
     [================================================================>]  Step: 18ms | Tot: 57s66ms | Test >> Loss: 0.317 | Acc: 90.700% (9070/10000) 2500/2500 
    
    Epoch: 29
     [================================================================>]  Step: 45ms | Tot: 10m39s | Train >> Loss: 0.119 | Acc: 95.856% (47928/50000) 12500/12500 
     [================================================================>]  Step: 21ms | Tot: 58s597ms | Test >> Loss: 0.297 | Acc: 91.110% (9111/10000) 2500/2500 
    ```

- Analysis

  - No of Epochs : 30
  - Best Train Acc: 95.856%
  - Best Test Acc: 91.110%
  - No Model changes in Resnet18
  - Extra things done
    - Normalization:
      - transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    - Image augmentation
      - Train
        - RandomCrop
        - RandomHorizontalFlip
      - Test
        - Nothing
  - Overfitting
    - Calculation: (100 - 91.110) + 95.856 = 104.746 --> No overfitting
    - Difference : 95.856 - 91.110 = 4.746 --> Less -> No overfitting
  - Train and test acc were converging till epoch 7 and started to diverge post that.
  - There has been a continous increase in train accuracy
  - Test accuracy is fluctuating
  - Gap between train and test acc is 4.7 in last epoch which is reasonable
  - The model has capacity to be trained further but accuracy gains won't be too much.
